{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Equivalent Circuit Bayesian Inference\n",
    "\n",
    "In this notebook, we introduce the Monte Carlo sampling methods for an example Thevenin model. This notebook includes importing experimental data for a Tesla 4680 NCA/Gr cell, and identifying the resistance elements of a two-RC Thevenin model. First, we import PyBOP and the required packages,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip ipywidgets -q\n",
    "%pip install pybop -q\n",
    "%pip install pandas -q\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pybamm\n",
    "\n",
    "import pybop\n",
    "\n",
    "pybop.plot.PlotlyManager().pio.renderers.default = \"notebook_connected\"\n",
    "parallel = True if sys.platform != \"win32\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Next, we will import and process the experimental data. The data for this notebook was obtained from the supplemental data provided in:  \n",
    "\n",
    "[1] M. Ank et al., ‘Lithium-Ion Cells in Automotive Applications: Tesla 4680 Cylindrical Cell Teardown and Characterization’, doi: 10.1149/1945-7111/ad14d0.\n",
    "\n",
    "A portion of this data is retained in the PyBOP repository for use in parameterisation and optimisation examples. However, users' are pointed to the official location https://mediatum.ub.tum.de/1725661 for more information. First, we import the three-electrode pOCV dataset,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocv_df = pd.read_csv(\n",
    "    \"../../data/Tesla_4680/T-cell_pOCV_data.txt\",\n",
    "    sep=\"\\t\",\n",
    "    decimal=\",\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Next, let's plot the terminal voltage to visualise the protocol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocv_df.plot(y=\"Ewe-Ece/V\", x=\"time/h\", kind=\"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "As we are aiming to identify the resistance elements in an equivalent circuit model, we will construct an OCV function from the discharge portion of the experimental data. This is done by filtering the dataframe and appending an additional point to ensure the OCV function has data across the operating region (i.e. up to 4.2V)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sto_measured = (\n",
    "    1\n",
    "    - ocv_df.loc[(ocv_df[\"I/mA\"] <= 0.0), \"Capacity/mA.h\"]\n",
    "    / ocv_df.loc[(ocv_df[\"I/mA\"] <= 0.0), \"Capacity/mA.h\"].iloc[-1]\n",
    ")\n",
    "V_measured = ocv_df.loc[(ocv_df[\"I/mA\"] <= 0.0), \"Ewe-Ece/V\"]\n",
    "V_measured.iloc[0] = 4.2  # Extend for improved interpolation (a bit of a fudge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocv(sto):\n",
    "    name = \"OCV\"\n",
    "    x = np.flip(sto_measured.to_numpy())\n",
    "    y = np.flip(V_measured.to_numpy())\n",
    "    return pybamm.Interpolant(x, y, sto, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Next, let's construct the parameter set and update it with the known information and placeholder values for this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_set = pybop.ParameterSet.pybamm(\"ECM_Example\")\n",
    "parameter_set.update(\n",
    "    {\n",
    "        \"Cell capacity [A.h]\": 22.651,  # 083/828 - C/20\n",
    "        \"Nominal cell capacity [A.h]\": 22.651,\n",
    "        \"Current function [A]\": 22.651,\n",
    "        \"Initial SoC\": 1.0,\n",
    "        \"Upper voltage cut-off [V]\": 4.25,  # Extended to avoid hitting event\n",
    "        \"Lower voltage cut-off [V]\": 2.5,\n",
    "        \"Open-circuit voltage [V]\": ocv,\n",
    "        \"R2 [Ohm]\": 1e-4,  # placeholder\n",
    "        \"C2 [F]\": 4e5,  # placeholder\n",
    "        \"Element-2 initial overpotential [V]\": 0,\n",
    "    },\n",
    "    check_already_exists=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Import Cycling Data\n",
    "Now we import the corresponding constant current tests for this cell and construct a dataset from a subset of this time-series. This is done to reduce the computational time for inference, but also to improve the robustness of the inference task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycling_df = pd.read_csv(\n",
    "    \"../../data/Tesla_4680/601-828_Capacity_03_MB_CB1_subset.txt\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "filter_cycling = cycling_df.loc[54811:61000].copy()  # Full cycle is [54811:127689]\n",
    "filter_cycling[\"time/s\"] = filter_cycling[\"time/s\"] - filter_cycling[\"time/s\"].iloc[0]\n",
    "\n",
    "# Take every 100th point\n",
    "filtered_cycling_df = filter_cycling.iloc[\n",
    "    [i for i in range(len(filter_cycling)) if not i % 100 != 0]\n",
    "]\n",
    "filtered_cycling_df.plot(x=\"time/s\", y=\"Ecell/V\", kind=\"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Defining the parameters for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pybop.Parameters(\n",
    "    pybop.Parameter(\n",
    "        \"R0 [Ohm]\",\n",
    "        prior=pybop.Gaussian(3e-3, 1e-3),\n",
    "        bounds=[1e-5, 1e-2],\n",
    "    ),\n",
    "    pybop.Parameter(\n",
    "        \"R1 [Ohm]\",\n",
    "        prior=pybop.Gaussian(5e-3, 1e-3),\n",
    "        bounds=[1e-5, 1e-2],\n",
    "    ),\n",
    "    pybop.Parameter(\n",
    "        \"R2 [Ohm]\",\n",
    "        prior=pybop.Gaussian(1e-4, 5e-5),\n",
    "        bounds=[1e-5, 1e-2],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Form the dataset using the filtered data\n",
    "In this parameter inference task, we use the filtered time-series data to reduce the computation time. This choice can be validated by comparing the inference results to the full time-series inference, which isn't covered in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hundred = pybop.Dataset(\n",
    "    {\n",
    "        \"Time [s]\": filtered_cycling_df[\"time/s\"].to_numpy(),\n",
    "        \"Current function [A]\": -filtered_cycling_df[\"I/mA\"].to_numpy()\n",
    "        / 1000,  # Convert mA to A\n",
    "        \"Voltage [V]\": filtered_cycling_df[\"Ecell/V\"].to_numpy(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Construct the model, problem, and non-scaled posterior\n",
    "\n",
    "We now construct a two-pair RC model and build the model with an initial SOC based on the first voltage point in the experimental data. \n",
    "The `FittingProblem` and likelihood classes are constructed with posterior built from the GaussianLogLikelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pybop.empirical.Thevenin(\n",
    "    parameter_set=parameter_set, options={\"number of rc elements\": 2}\n",
    ")\n",
    "model.build(\n",
    "    initial_state={\n",
    "        \"Initial open-circuit voltage [V]\": dataset_hundred[\"Voltage [V]\"][0]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate problem, likelihood, and sampler\n",
    "problem = pybop.FittingProblem(model, parameters, dataset_hundred)\n",
    "likelihood = pybop.GaussianLogLikelihood(problem)\n",
    "posterior = pybop.LogPosterior(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Let's find the Maximum a Posteriori values\n",
    "Below we identify the parameters using the Maximum a Posterior estimate and the Covariance Matrix Adaptation Evolution Strategy (CMAES). We will use this estimate later in the Bayesian inference task as a starting position for the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = pybop.CMAES(\n",
    "    posterior,\n",
    "    sigma0=[1e-4, 1e-4, 1e-4, 1e-4],\n",
    "    max_iterations=200,\n",
    "    max_unchanged_iterations=40,\n",
    "    parallel=parallel,\n",
    ")\n",
    "results_optim = optim.run()\n",
    "print(results_optim)\n",
    "pybop.plot.quick(problem, results_optim.x)\n",
    "pybop.plot.convergence(optim)\n",
    "pybop.plot.parameters(optim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7583eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = results_optim.x\n",
    "y,dy = problem.evaluateS1(optimal_params)\n",
    "fim = likelihood.observed_fisher(y,dy)\n",
    "print(\"Observed Fisher Information Matrix:\",fim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Monte Carlo Sampling\n",
    "Below we construct the Monte Carlo sampler, specifically the Hamiltionian sampler, which samples from the posterior using Hamiltonion dynamics with gradient information. To minimise the time to execute this notebook, we greatly limit the number of samples; however, this should be increased for actual inference tasks. We initialise the chains from the Maximum a Posteriori point-estimate obtained from the CMAES optimisation above, as this should be close to the mode of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = pybop.HaarioBardenetACMC(\n",
    "    posterior,\n",
    "    chains=3,\n",
    "    x0=results_optim.x,  # Initialise at the MAP estimate\n",
    "    max_iterations=1500,  # Increase for accurate posteriors\n",
    "    warm_up=350,\n",
    "    verbose=True,\n",
    "    parallel=parallel,  # Only supported for macOS/WSL/Linux\n",
    ")\n",
    "\n",
    "chains = sampler.run()\n",
    "\n",
    "# Create summary statistics\n",
    "posterior_summary = pybop.PosteriorSummary(chains)\n",
    "\n",
    "# Create rhat of chains\n",
    "print(posterior_summary.rhat())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "Next, we plot the parameter traces as well as the combined posterior (across all chains) and each chain individually. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_summary.plot_trace()\n",
    "posterior_summary.plot_posterior()\n",
    "posterior_summary.plot_chains()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "As expected, these chains haven't fully converged yet, so the values obtained from the posterior will be biased based on the initial conditions for sampling. Increasing the number of iterations for the sampler while also calibrating the `warm_up` period will provide better results. Next, for completeness, we will plot the identified time-series model against the experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybop.plot.quick(problem, posterior_summary.mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Now, we can compare the identified parameters for each method. As the Bayesian sampler provides us with samples from the posterior, we use the statistical moments when comparing to the point-based result from the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The XNES result is: {results_optim.x}\")\n",
    "print(f\"The posterior means are:{posterior_summary.mean}\")\n",
    "print(\n",
    "    f\"The difference between the two methods is: {np.abs(results_optim.x - posterior_summary.mean)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "The sampled posterior also gives us information of the uncertainty in the inference process. This is present in the standard deviations as well as the lower and upper confidence intervals, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_summary.summary_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
